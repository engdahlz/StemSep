[page 586/642]
    musdb18hq+moises+original  dataset  (41  songs  (In  total,  432  tracks,  16.5GB,  FLAC,  very  
small
 
dataset,
 
OG
 
was
 
trained
 
on
 
5K
 
songs)
 
~becruily
 Used  config  to  finetune:  https://drive.google.com/file/d/1gK1_n_bpRHD1i02VA2bgUc3TrpEJUcg9/view?usp=drive_link train.py  with  optimizer  (probably  it's  already  integrated  into  MSST  code):  https://drive.google.com/file/d/1jLSTDajYxZRSb5wLOwyVuRJrayNLIWUZ/view?usp=sharing  Changes  were  pushed  to  ZFTurbo  training  dataset,  but  the  optimizer  turned  out  to  not  save  too  much  VRAM  (diagram).   “'adamw8bit'  for  training.optimizer  in  config).  Also  added  possibility  to  provide  optimizer  
parameters
 
in
 
config
 
file
 
using
 
keyword
 
'optimizer'.”
 
ZFTurbo
 
Optimizers
 
explained
 
later
 
below.
  And  it’s  not  enough  to  point  the  model  trained  by  unwa  turned  out  to  have  the  same  0  SDR  
issue
 
becruily
 
had,
 
but
 
unwa
 
didn’t
 
notice
 
it
 
due
 
to
 
lack
 
of
 
validation
 
dataset.
 
So
 
inference
 
worked
 
correctly
 
despite
 
0
 
SDR,
 
but
 
the
 
model
 
was
 
getting
 
worse
 
gradually
 
during
 
finetuning.
  Frazer  suggestion  (probably  already  implemented  by  ZFTurbo):  “I  think  the  issue  with  the  checkpointing  /  0  SDR  bug  is  due  to    
|At
 
least
 
one
 
input
 
and
 
output
 
must
 
have
 
requires_grad=True
 
for
 
the
 
reentrant
 
variant.
 
If
 
this
 
|condition
 
is
 
unmet,
 
the
 
checkpointed
 
part
 
of
 
the
 
model
 
will
 
not
 
have
 
gradients.
 
The
 
|non-reentrant
 
version
 
does
 
not
 
have
 
this
 
requirement.
   So  I  think  the  fix  is  either  assigning   x.requires_grad=True   in  train.py  to  the  batch  tensor  before  passing  into  the  model   or  passing   use_reentrant=False   to  all  torch.utils.checkpoint.checkpoint  calls  I  think  it's  probably  better  to  use  the  non-reentrant  variant,  since  Pytorch  will  default  to  this  in  
later
 
versions
 https://pytorch.org/docs/2.1/checkpoint.html#torch.utils.checkpoint.checkpoint also  this  point  here  looks  interesting  to  test  whether  it  causes  a  significant  performance  hit  or  
not
 |The  logic  to  stash  and  restore  RNG  states  can  incur  a  moderate  performance  hit  depending  
|on
 
the
 
runtime
 
of
 
checkpointed
 
operations.
 
If
 
deterministic
 
output
 
compared
 
to
 
|non-checkpointed
 
passes
 
is
 
not
 
required,
 
supply
 
preserve_rng_state=False
 
to
 
checkpoint
 
or
 
|checkpoint_sequential
 
to
 
omit
 
stashing
 
and
 
restoring
 
the
 
RNG
 
state
 
during
 
each
 
|checkpoint.”
  
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  