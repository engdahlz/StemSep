[page 587/642]
    -  It  was  discovered,  that  using  different  chunk_sizes  at  various  stages  of  training  can  be  
beneficial
 
(iirc
 
esp.
 
for
 
training
 
time
 
at
 
initial
 
stages
 
without
 
much
 
SDR
 
sacrifice).
  -  “I  added  code  to  train  using  accelerate  module:  https://github.com/ZFTurbo/Music-Source-Separation-Training/blob/main/train_accelerate.py It's  useful  on  multi  GPU  systems.  In  my  experiments,  speed  improved  ~50%.  ~1.57  sec  per  iteration  goes  down  to  ~1.07  sec  per  iteration.   But  I  think  the  script  has  some  flaws  -  my  validation  score  during  training  is  lower  than  in  
reality.
 
I
 
didn't
 
find
 
the
 
reason
 
yet.
 Also,  script  allows  training  across  multiple  machines  without  changes  in  code.   More  information  here:  https://huggingface.co/docs/accelerate/index https://huggingface.co/docs/accelerate/quicktour”  -  ZFTurbo   -  What  unwa  said  later  in  October  2024  what  allowed  the  fine-tunes  to  be  made  on  8GB  
VRAM
 
and
 
RTX
 
3070,
 
is
 
they
 
used
 
gradient
 
checkpointing
 
-
 
“time
 
and
 
space
 
are
 
a
 
trade-off,
 and  gradient  checkpointing saves  memory  at  the  expense  of  computation  time”  Q:  And  you  don't  get  the  0  SDR  issue?  A:  “Yes.  I'm  using  L1Freq  metric  now.  As  it  turns  out,  it  was  not  a  failure  to  train  properly,  but  
just
 
a
 
problem
 
with
 
the
 
validation
 
function.”
 
-
 
unwa
 More on  the  issue.  “The  validation  issue  should  be  solved  now  [in  valid.py]  but  not  sure  if  it  was  the  same  issue  ZFTurbo  was  facing”  
 -  “https://huggingface.co/pcunwa/Mel-Band-Roformer-small In  the  experiments  with  the  Mel-Band  Roformer  big  model,  it  was  confirmed  that  increasing  
the
 
number
 
of
 
parameters
 
for
 
the
 
Mask
 
Estimator
 
did
 
not
 
improve
 
performance.
 Therefore,  I  conducted  an  experiment  to  see  if  I  could  reduce  the  number  of  parameters  
while
 
maintaining
 
the
 
performance.
 It  even  runs  well  on  4  GB  cards  due  to  the  reduced  memory  used.”  -  unwa   ZFTurbo:  “I  looked  onto  unwa  code  for  small  Roformers.  Roformers  have  one  parameter  
mlp_expansion_factor
 
which
 
couldn't
 
be
 
change[d]
 
from
 
config
 
and
 
fixed
 
as
 
4.
 
It
 
uses
 
a
 
lot
 
of
 
memory:
  │     └─MaskEstimator:  2-8                                [1,  1101,  7916]            --  │     │     └─ModuleList:  3-73                             --                         201,465,304   if  set  to  1  (memory  reduced  10  times  200  MB  to  23  MB):   │     └─MaskEstimator:  2-8                                [1,  1101,  7916]            --  │     │     └─ModuleList:  3-73                             --                         23,836,120  Yesterday  I  already  added  in  my  repository  possibility  to  change  mlp_expansion_factor  from  
config.
 
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  