[page 628/642]
    https://www.unite.ai/mamba-redefining-sequence-modeling-and-outforming-transformers-architecture/  https://github.com/state-spaces/mamba https://github.com/apapiu/mamba_small_bench (“this  one  is  actually  exciting  because  it  runs  faster  and  leaner  than  transformers  and  
promises
 
to
 
surpass
 
them
 
in
 
quality
 >What  makes  Mamba  truly  unique  is  its  departure  from  traditional  attention  and  MLP  blocks.  
This
 
simplification
 
leads
 
to
 
a
 
lighter,
 
faster
 
model
 
that
 
scales
 
linearly
 
with
 
the
 
sequence
 
length
 
–
 
a
 
feat
 
unmatched
 
by
 
its
 
predecessors.
 
Mamba
 
has
 
demonstrated
 
superior
 
performance
 
in
 
various
 
domains,
 
including
 
language,
 
audio,
 
and
 
genomics...”)
 “mamba  is  real  fucking  complicated.  like  reaaaally  complicated  (...)  hyper  params  do  seem  
hard
 
to
 
adjust
 
tho.”
 “mamba  is  kinda  sick  but  its  early  days  in  the  SSM  space,  so  lots  of  the  tricks  that  you  can  
do
 
with
 
transformers
 
you
 
can't
 
do
 
with
 
SSMs
 
because
 
they
 
haven't
 
become
 
mainstream
 but  mamba  has  two  very  cool  properties  it  has  positional  information  by  its  nature  -  i.e.  no  extra  computation  is  required  to  embed  
positional
 
info
 linear  time  complexity  -  so  in  audio  it's  super  useful  because  audio  data  hits  the  On^2  
complexity
 
(if
 
the
 
chunksize
 
is
 
large
 
enough)”
 “i  personally  don't  trust  any  of  the  mamba  papers  -  they  either  say  how  mamba  is  the  best  
thing
 
since
 
sliced
 
bread
 
or
 
worse
 
than
 
3
 
year
 
old
 
transformers
 although  the  paper  I  read  for  that  was  questionable”  https://www.harvard.edu/kempner-institute/2024/02/02/repeat-after-me-transformers-are-better-than-state-space-models-at-copying-2/”  “they  don't  even  replace  the  mask  estimator  thing  in  bs-roformer  with  mamba”  https://arxiv.org/abs/2404.02063  https://arxiv.org/abs/2401.09417 https://github.com/hustvl/Vim https://github.com/RobinBruegger/RevTorch https://huggingface.co/blog/rwkv  Why  does  music  source  separation  benefit  from  cacophony?  https://arxiv.org/abs/2402.18407 It  makes  our  side  chain  stem  limiting  thing  irrelevant.  “As  the  paper  demonstrate  that  using  only  randomly  mixed  stems  is  more  efficient  for  
training
 
than
 
using
 
only
 
real
 
paired
 
stems
 
(from
 
the
 
same
 
song
 
and
 
sync),
 
in
 
that
 
random
 
mix
 
config,
 
the
 
individual
 
stems
 
will
 
never
 
be
 
against
 
the
 
mixture
 
that
 
was
 
used
 
to
 
limit
 
them,
 
so
 
making
 
that
 
process
 
irrelevant”
 
jarredou
 MDX23C  training  code  by  ZFTurbo  has  the  mix  randomization  feature  built-in  -  dataset  type  
1
 
is
 
random
 
mix,
 
dataset
 
type
 
4
 
is
 
the
 
real
 
mix
 
(aligned).
 “I  think  now  after  reading  that  paper  that  once  you  have  a  dataset  large  enough  and  using  
the
 
random
 
mixing
 
with
 
some
 
simple
 
augmentations
 
like
 
gain
 
changes/channel
 
swap/phase
 
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  