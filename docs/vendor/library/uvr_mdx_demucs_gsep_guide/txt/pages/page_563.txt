[page 563/642]
    Larger  batch  size  will  make  the  model  not  so  good,  because  it  has  to  learn  bigger  passages  
at
 
once,
 
but
 
the
 
model
 
will
 
train
 
faster.
 You  need  to  tweak,  balance  out  and  find  what  works  for  you  the  best  for  a  model  you’re  
training.
 
Also
 
balancing
 
things
 
out
 
might
 
be
 
helpful
 
for
 
end
 
users
 
with
 
slower
 
GPUs,
 
or
 
even
 
CPUs
 
[although
 
bigger
 
MDX23C
 
(v3)
 
models
 
are
 
very
 
difficult
 
to
 
separate
 
on
 
CPU,
 
nearly
 
impossible
 
on
 
the
 
oldest
 
4
 
cores
 
and
 
still
 
noticeably
 
slower
 
than
 
MDX-Net
 
models
 
on
 
GPUs
 
like
 
3050].
 The  section  continues  later  below.   Overfitting  “Is  when  a  model  is  still  improving  on  training  data  but  not  on  unseen  data,  and  if  training  is  
push
 
too
 
far,
 
it
 
can
 
even
 
start
 
to
 
perform
 
worse
 
on
 
unseen
 
data.
 It's  more  important  issue  when  you  want  a  model  that  generalise  well”,  [e.g.  targeting  only  
909
 
hihats],
 
you
 
want
 
a
 
model
 
which
 
targets
 
one
 
really
 
precise
 
sound
 
(with
 
some
 
variation,
 
but
 
still
 
909
 
hihats,
 
so
 
it's
 
not
 
really
 
about
 
generalisation.”
 
jarredou
  In  terms  of  training,  currently  Anjok  uses  A6000  48GB  and  Ryzen  7  5800,  128GB  RAM,  3TB  
NVME,
 
you
 
need
 
an
 
SSD
 
for
 
training
 
as
 
the
 
training
 
process
 
in
 
intensive
 
for
 
a
 
massive
 
amount
 
of
 
data.
  MDX23C   Noticeably  slower  for  separation  than  MDX-Net,  even  for  GPUs  like  3050.  3000  samples  of  3-4  minutes  length,  it's  going  to  take  at  least  for  batch  size  of  8,  a  month  
and
 
a
 
half
 
(?on
 
A6000
 
and
 
MDX-Net).
 
Anjok
 
didn't
 
want
 
to
 
make
 
models
 
too
 
big,
 
having
 
end
 
users
 
with
 
not
 
the
 
best
 
hardware
 
in
 
mind
 
(hence
 
the
 
choice
 
of
 
the
 
older
 
arch).
 (here  the  interview  section  ends)   Everything  should  be  trained  to  min.  200  epochs  (at  least  for  a  model  trained  from  scratch),  
and
 
better,
 
for
 
500
 
(e.g.
 
MDX-Net
 
HQ_2
 
was
 
trained
 
to
 
450
 
epochs).
 
From
 
e.g.
 
200
 
upward,
 
the
 
increase
 
of
 
SDR
 
can
 
be
 
very
 
low
 
for
 
a
 
longer
 
time.
 
Experimentally,
 
HQ_4
 
was
 
trained
 
to
 
epoch
 
1149,
 
and
 
it
 
slowly,
 
but
 
consequently
 
progressed
 
further
 
beyond.
 
In
 
general,
 
some
 
people
 
train
 
models
 
up
 
to
 
750
 
or
 
1000
 
epochs,
 
indeed,
 
but
 
it
 
takes
 
longer.
  Somewhere  at  the  beginning  of  2023,  UVR  dataset  consisted  of  2K  songs  (maybe  for  voc_ft,  
can’t
 
remember),
 
and
 
probably
 
more
 
for
 
MDX23C,
 
and
 
700
 
pairs
 
for
 
BVE
 
model,
 
but
 
in
 
case
 
of
 
vocal
 
model,
 
the
 
one
 
with
 
7K
 
songs
 
didn't
 
achieve
 
much
 
better
 
SDR
 
results
 
than
 
2K.
 
Could've
 
been
 
a
 
problem
 
of
 
overfitting
 
or
 
no
 
cutoff
 
for
 
vocal
 
model
 
or
 
any
 
other
 
problem
 
with
 
dataset
 
creation
 
we
 
will
 
tackle
 
here
 
later.
  The  best  publicly  available  archs  for  training  instrumentals/vocals  which  community  already  
used,
 
are:
 
 MelBand  Roformer  (faster  and  can  surpass  MDX23C  and  BS-Roformer  SDR-wise  with  e.g.  
Kim
 
config
 
below
 
[and
 
not
 
only],
 
and
 
can
 
sound
 
better
 
and
 
less
 
muddy
 
than
 
BS),
 
BS-Roformer
 
(very
 
demanding,
 
better
 
for
 
specific
 
tasks),
 
MDX23C
 
(can
 
produce
 
more
 
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  