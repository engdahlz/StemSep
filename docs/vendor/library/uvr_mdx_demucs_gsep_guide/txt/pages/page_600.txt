[page 600/642]
     But  if  you  have  enough  time  at  hand,  RTX  4090  is  cheaper  in  the  long  run.  Depends  on  your  electricity  costs,  though,  which  varies  per  country.   Training  and  inference  performance  for  GPU  per  dollar  https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png?ssl=1  Be  aware  that  multi  GPU  configurations  don’t  scale  linearly.   We  had  an  interesting  discussion  on  the  server  on  the  choice  between  GTX  1080  Ti  vs  RTX  
3060
 
12GB
 
in
 
training.
 
We’re
 
yet
 
to
 
find
 
out
 
the
 
final
 
result,
 
but
 
unwa
 
claims
 
that
 
despite
 
having
 
more
 
CUDA
 
cores,
 
1080
 
Ti
 
might
 
turn
 
out
 
to
 
be
 
slower.
 
The
 
possible
 
reasons:
 -   Pascal’s  are  “limited  by  FP16  performance  by  a  factor  of  64”  -  No  tensor  cores  (“Normally,  AMP  (Automatic  Mixed  Precision)  is  turned  on  when  training  a  
model,
 
but
 
AMP
 
uses
 
Tensor
 
Core
 
to
 
speed
 
up
 
the
 
computation.”
 
plus
 
“Tensor
 
Core
 
generation
 
is
 
also
 
newer
 
[in
 
3000
 
series],
 
with
 
support
 
for
 
more
 
precisions,
 
including
 
BF16.”)
 
-
 
“3060
 
is
 
one
 
generation
 
newer
 
than
 
RTX
 
20xx
 
/
 
GTX
 
16xx
 
and
 
can
 
use
 
Flash
 
Attention2.
 
This
 
is
 
not
 
very
 
relevant
 
for
 
music
 
source
 
separation
 
model,
 
but
 
may
 
be
 
very
 
useful
 
for
 
LLM
 
inference.
 
(Roformer
 
models
 
have
 
a
 
Flash
 
Attention
 
entry
 
in
 
the
 
configuration,
 
but
 
Memory
 
Efficient
 
Attention
 
is
 
used
 
unless
 
A100
 
GPU(s)
 
are
 
used.)”
 Bas  Curtiz  is  probably  yet  to  find  out  the  final  verdict.   -  “This  is  an  extreme  example,  but  it's  a  table  comparing  video  generation  times  for  each  
GPU
 
under
 
the
 
settings
 
of
 
Wan2.2
 
Q6
 
K,
 
1280x704,
 
84
 
seconds,
 
and
 
8
 
steps.
 
The
 
5060Ti
 
16GB
 
even
 
outperforms
 
the
 
3090”
 
-
 
Unwa
 pic   Q:  How  long  it  takes  to  train  a  model?  A:  "Depends  on  input  and  parameters  and  architecture.  MDX  old  version:  5k  input  (15k  actually:  inst/mixture/vocals)  +  100  validation  tracks  (300,  same  deal),  fullband,  
300
 
epochs
 
would
 
have
 
taken
 
3
 
months
 
on
 
a
 
RTX
 
4090.
  You  can  speed  it  up  by  going  multiple  GPUs  and  more  memory,  therefore:  A6000  (48gb)  x  8  was  like  14  days.  Damage  on  300  epochs:  ~700  bucks."   "7  days  training  of  e.g.  BS-Roformer  on  8xA100-80GB":  7\*24\*15.12  (runpod  8xa100  
pricing)
 
=
 
$2540.16”
 4  days  achieved  epoch  74,  and  on  epoch  162  for  ~4200/4500  songs    Q:  “4070  [8GB]  works,  but  I  would  only  use  for  testing  IMO  
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  