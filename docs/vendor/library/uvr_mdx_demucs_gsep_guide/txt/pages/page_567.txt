[page 567/642]
    A:  Absolutely  not.  It's  merely  indication    SDR  measurement  is  logarithmic,  meaning  that  1  SDR  is  10x  difference.   Q:  Why  I  have  negative  SDR  values  (based  on  HTDemucs)  A:  Make  sure  there  are  no  empty  stems  in  any  training  dataset  and  or  validation  dataset    Below  is  just  a  theory  for  now  and  probably  wasn't  strictly  tested  on  any  model  yet,  but  
seems
 
promising
 
  Q:  Can  you  not  calculate  the  average  dB  of  the  stems  and  fit  one  limiting  value  to  them  all?  A:  the  stems  are  divide-maxed  prior  meaning  they  are  made  so,  that  when  joined  together,  they  won't  clip  but  are  normalized  so  they  will  be  kinda  standardized  already  based  on  that,  I  should  be  able  to  just  go  with  one  static  value  for  all  Example  https://www.youtube.com/watch?v=JYwslDs-t4k Q:  This  is  great,  I  actually  used  this  method  before  with  a  few  sets  of  stems,  before  I  decided  
to
 
try
 
sidechain
 
compression/
 
Voxengo
 
elephant
 
method,
 
but
 
I'm
 
not
 
too
 
sure
 
if
 
I
 
am
 
on
 
the
 
right
 
path.
 
However,
 
I'm
 
pretty
 
sure
 
this
 
only
 
works
 
best
 
for
 
evaluation,
 
if
 
the
 
resulting
 
mixture
 
has
 
consistent
 
loudness
 
like
 
in
 
today's
 
music.
 A:  Yeah,  it's  a  different  approach  than  compression/voxengo  indeed.  But  the  fact  it  scored  high  in  SDR  and  UVR  dataset  is  already  compressed/elphanted  I  think  it's  a  good  combo  to  use  both  in  the  set,  a  bit  like  new  style  tracks  and  oldies  [so  to  
use
 
both
 
approaches
 
inside
 
the
 
dataset]
 some  tracks  in  real  life  are  compressed  like  fuck  -  some  aren't  so  it  mimics  real  life  situation  Q:  if  it's  true  that's  awesome,  with  that  the  model  basically  has  the  potential  to  work  in  
multiple
 
mixing
 
styles,
 
without
 
having
 
to
 
create
 
new
 
data,
 
or
 
changing
 
it,
 
right?
 While  still  adding  new  data  A:  Yeah,  since  UVR  dataset  is  already  compressed  -  and  then  add  these  one  of  mines  with  
the
 
more
 
delicate
 
way
 
of
 
mastering
 
(incl.
 
divdemax
 
prior)
  Q:  Does  somebody  know  the  best  way  to  make  dataset  smaller?  I  have  very  huge  dataset  in  
flac
 
format,
 
so
 
the
 
one
 
idea
 
is
 
to
 
truncate
 
part
 
in
 
the
 
song
 
where
 
is
 
only
 
music
 
without
 
vocals?
 
Also,
 
I
 
can
 
convert
 
it
 
to
 
opus
 
format,
 
does
 
it
 
worse
 
it?
 
Or
 
maybe
 
there
 
is
 
something
 
better
 
that
 
I
 
don't
 
know?
 A  (jarredou):  If  you  plan  to  use  random  mixing  of  stems  during  training  (so  non-aligned  
dataset),
 
then
 
you
 
can
 
remove
 
all
 
silent
 
parts
 
from
 
stems
 
pre-training,
 
on
 
instrumental
 
it
 
will
 
not
 
change
 
a
 
lot
 
but
 
for
 
vocals
 
it
 
can
 
save
 
a
 
lot
 
of
 
space
 
(h/t
 
Bas
 
Curtiz
 
for
 
the
 
idea)
 Q:  Currently  dataset  is  aligned,  but  does  this  random  mixing  is  standard  approach?  I  am  
going
 
to
 
train
 
official
 
SCNet
 
model,
 
so
 
maybe
 
it
 
will
 
require
 
modifications
 
for
 
this?
 
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  