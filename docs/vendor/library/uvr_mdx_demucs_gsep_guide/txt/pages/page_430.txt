[page 430/642]
    Using  other  models  not  included  in  the  Colab  -  https://github.com/jarredou/MVSEP-MDX23-Colab_v2/blob/v2.5/inference.py E.g.  in  line  452  you  can  replace  Kim  model  by  any  other  vocal  model,  and  replace  that  
edited
 
in
 
file
 
manager
 
once
 
Colab
 
has
 
executed
 
initialization
 
cell
 
or
 
fork
 
the
 
repo.
 
As
 
for
 
using
 
instrumental
 
Roformer
 
models
 
instead
 
of
 
vocal
 
models,
 
I
 
can't
 
guarantee
 
it
 
will
 
work
 
correctly.
 -  “Easiest  way  [to  replace  MDX  HQ  model]  should  be  to  replace  Inst-HQ4  link  with  HQ5  link  
line
 
480
 https://github.com/jarredou/MVSEP-MDX23-Colab_v2/blob/36909309efd4a75dab9f1d093a112785a8f560fb/inference.py#L480 If  models  parameters  are  same  (iirc  they  are),  drop-in  replacement  should  work  (and  then  
you
 
control
 
HQ5
 
with
 
HQ4
 
settings
 
in
 
colab
 
GUI)”
 -  Change  the  args  awaited  by  inference.py  accordingly  to  the  ones  you've  changed  in  the  
Colab
 
notebook
 
[if
 
you
 
decide
 
to
 
change
 
models
 
names
 
in
 
he
 
Colab],
 
it's
 
at
 
bottom
 
of
 
inference.py
 
(line
 
874
 
and
 
so
 
on)
 -  Adding  e.g.  SCNet  is  not  that  easy  task,  it  will  also  require  to  really  add  SCNet  arch  to  the  
script,
 
not
 
only
 
words
 
(add
 
its
 
core
 
files
 
to
 
"modules"
 
folder,
 
import
 
them
 
in
 
main
 
script,
 
check
 
if
 
that
 
work
 
with
 
existing
 
"demix"
 
functions,
 
etc...
 
else
 
it
 
can't
 
work).
 https://github.com/ZFTurbo/Music-Source-Separation-Training/tree/main/models/scnet You  can  study  how  ZFTurbo  is  doing  it  with  his  script  and  then  try  to  adapt  it  to  MDX23  
Colab.
 
~jarredou
   -  What  weight  you  should  use  for  your  custom  model?  “You  must  process  an  evaluation  dataset  with  each  model  individually,  download  the  separated  audio  and  then  use  my  "weight  finder"  script  (here [mirrored])  with  all  the  separated  audios  from  each  model.  It  will  try  many  different  weights  until  it  find  the  best  ones  
for
 
the
 
given
 
model
 
inputs.
 Else  you  can  set  "random"  weights,  process  the  multisong  dataset  from  MVSEP  and  upload  
the
 
separated
 
audios
 
to
 
the
 
quality
 
checker
 
to
 
get
 
the
 
evaluation
 
scores
 https://mvsep.com/en/quality_checker (and  repeat  until  you're  satisfied)  Download  the  mutlisong  eval  dataset  provided  on  the  quality  checker  link  I've  shared  above.  
Process
 
the
 
100
 
tracks
 
with
 
the
 
model/ensemble
 
you
 
want
 
to
 
evaluate.
 
Download
 
the
 
separated
 
audio.
 Rename  the  files  accordingly  to  guidelines  provided  in  quality  checker  link,  zip  them,  upload  
them
 
and
 
wait
 
for
 
the
 
results
 All  in  same  folder,  and  named:  song_000_instrum.wav  song_000_vocals.wav  song_001_instrum.wav  song_001_vocals.wav  song_002_instrum.wav  song_002_vocals.wav  etc...  
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  