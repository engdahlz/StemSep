[page 574/642]
        --data_path  '/content/drive/MyDrive/TRAININGDATASET'  \      --valid_path  '/content/drive/MyDrive/VALIDATIONDATASET'  \      --num_workers  4  \      --device_ids  0   Don't  forget  to  edit  the  config  file  for  training  parameters  
 
You
 
can
 
also
 
resume
 
training
 
from
 
an
 
existing
 
checkpoint
 
by
 
adding
 
 --start_check_point  'PATH/TO/checkpoint.ckpt'  \  parameter  to  the  command  in  the  training  cell    the  checkpoints  are  saved  in  the  path  provided  by  the  :  --results_path  results/  \  parameter  of  the  command,  so  here,  in  "results"  folder   With  ZFTurbo's  script,  mixtures  are  needed  for  validation  dataset,  to  evaluate  epoch  
performance”
 
-
 
jarredou
  “it  saves  every  checkpoint  as  "last_archname.ckpt"  (file  is  overwritten  at  each  epoch),  and  
also
 
save
 
each
 
new
 
best
 
checkpoint
 
on
 
validation
 
as
 
"archname_epxx_SDRscore.ckpt".
  It  also  lowers  the  learning  rate  when  validation  eval  is  stagnant  for  a  chosen  number  of  
epochs
 
(reduceonplateau),
 
you
 
can
 
tweak
 
the
 
values
 
in
 
model
 
config
 
file.”
  Q:  what  does  this  gradient  accumulation  step/grad  clip  mean  exactly?   A:  “Accumulation  lets  you  train  with  a  larger  batch  size  than  what  you  can  fit  on  your  GPU,  
your
 
real
 
batch
 
size
 
will
 
be
 
batch_size
 
multiplied
 
by
 
gradient_accumulation_steps.
 
  grad_clip  clips  the  gradients,  it  can  stop  the  exploding  gradients  problem   Exploding  gradients  =  model  ruined  basically,  i  had  this  problem  with  Demucs  training,  but  I  
used
 
weight
 
decay
 
(AdamW)
 
to
 
solve
 
it
 
instead
 
of
 
grad_clip
  I  don't  think  grad_clip  uses  any  resources,  but  accumulation  uses  a  little  bit  of  VRAM,  i  don't  
know
 
the
 
exact
 
number”
 
-
 
Kim
  Q:  Why  can’t  models  have  like  an  auto  stop  feature  or  something  IDK  like  if  the  model  stops  
improving
 
it’ll
 
stop
 
automatically
 or  overtraining,  but  IDK  if  models  can  overtrain   A:  Nothing  stopping  you  from  adding  a  thing  to  stop  training  after  seeing  SDR  (or  whatever)  
is
 
stagnant,
 
some
 
people
 
even
 
represent
 
it
 
in
 
a
 
chart
 
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  