[page 578/642]
    A:  “That's  what  I've  done  with  my  latest  version.   Duplications  “kinda  back,  differently  but  still  can  lead  to  similar  audible  ringing  artifacts  with  
some
 
configs.
 
Taking
 
it
 
from
 
the
 
opposite
 
side:
 
if
 
it's
 
the
 
low
 
band
 
high
 
energy
 
the
 
issue,
 reduce  the  low  band  high  energy  (using  preemphasis  and  deemphasis  -  pic).  It  seems  to  get  rid  of  the  duplicated  stuff  and  ringing  artifacts  at  training  start.   
  Colab  for  training  MDX23C  model  (on  free  T4)  “Can't  train  multistem  model  because  of  limited  resources  of  Colab,  so  it's  one  by  one.   It's  only  1x  Tesla  T4,  15GB  VRAM  so  lots  of  [GPUs]  can  be  [much]  better!  I  can  run  batch_size  =  8  with  it  (with  n_fft=2048  instead  of  8192  in  the  model  config;  other  archs  are  using  n_fft=2048  too  
[Demucs,
 
Rofos…]).
 When  you  use  full  runtime  credits  one  day,  the  day  after,  you  get  only  1h10min  GPU  time  (2  
epochs.
  1.  It's  really  boring  to  do  2.  You  must  have  multiple  Google  accounts  3.  You  must  have  a  dataset  and  host  it  on  GDrive  and  share  it  with  all  the  accounts  (and  
making
 
it
 
accessible
 
at
 
root
 
for
 
each
 
account)
 4.  Use  this  fork  of  ZFTurbo's  training  script  that  is  allowing  better  resuming,  which  is  required  
for
 
Colab
 
as
 
sessions
 
are
 
deleted
 
after
 
3h~4h
 
max
 
(often
 
less)
 https://github.com/jarredou/Music-Source-Separation-Training/tree/wandb%2Bresume 5.  Edit  this  config  baseline  accordingly  to  your  dataset/needs  (click)  6.  Set  parameters  accordingly,  gdrive  connection  and  run.  7.  When  you've  burnt  all  credits  from  one  account,  switch  and  rerun.  When  you've  burnt  all  
credits
 
from
 
one
 
account,
 
switch
 
and
 
rerun.
 
When
 
you've
 
burnt
 
all
 
credits
 
from
 
one
 
account,
 
switch
 
and
 
rerun.
 
When
 
you've
 
burnt
 
all
 
credits
 
from
 
one
 
account,
 
switch
 
and
 
rerun.
 
When
 
you've
 
burnt
 
all
 
credits
 
from
 
one
 
account,
 
switch
 
and
 
rerun.
 
When
 
you've
 
burnt
 
all
 
credits
 
from
 
one
 
account,
 
switch
 
and
 
rerun.
 
When
 
you've
 
burnt
 
all
 
credits
 
from
 
one
 
account,
 
switch
 
and
 
rerun.
 
When
 
you've
 
burnt
 
all
 
credits
 
from
 
one
 
account,
 
switch
 
and
 
rerun.
 
When
 
you've
 
burnt
 
all
 
credits
 
from
 
one
 
account,
 
switch
 
and
 
rerun.
 
When
 
you've
 
burnt
 
all
 
credits
 
from
 
one
 
account,
 
switch
 
and
 
rerun.
 
When
 
you've
 
burnt
 
all
 
credits
 
from
 
one
 
account,
 
switch
 
and
 
rerun....
 
and
 
do
 
that
 
7
 
loop
 
for
 
weeks.
  I've  started  this  experiment  just  to  see  if  a  lightweight  mdx23c  model  could  be  trained  with  
free
 
Colab,
 
but
 
as
 
I
 
saw
 
it
 
was
 
quickly
 
achieving
 
higher
 
SDR
 
than
 
drumsep
 
on
 
my
 
tiny
 
eval
 
dataset,
 
I'm
 
continuing
 
the
 
training,
 
it's
 
almost
 
at
 
18SDR
 
now
 
for
 
kick”
 
-
 
jarredou
  Q:  Any  particular  reason  why  the  num  steps  is  1268  instead  of  1000?  Is  that  a  random  
number
 
or
 
a
 
calculation?
 
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  