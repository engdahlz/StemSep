[page 456/642]
    To  get  potentially  better  results  with  the  model  “try  using  a  small  pitch  shift  up  or  down,  like  
+/-
 
1
 
or
 
2
 
semitones,
 
in
 
the
 
settings
 
you
 
use
 
to
 
extract
 
the
 
drum
 
stem
 
from
 
the
 
instrumental
 
stem.
 
(...)
 
can
 
sometimes
 
help
 
bring
 
out
 
the
 
lows
 
or
 
highs
 
if
 
they
 
seem
 
weak.”
 
(CZ-84)
  It  can  already  be  used,  but  training  is  not  fully  finished  yet.  The  config  allows  training  on  not  so  big  GPUs  [n_fft  2048  instead  of  8096],  it's  open  to  
anyone
 
to
 
resume/fine-tune
 
it.
  For  now,  it's  struggling  a  bit  to  differentiate  ride/hh/crash  correctly,  kick/snare/toms  are  more  
clean.
 [“and  has  the  usual  issues  with  mdx23  models,  but  it’s  an  improvement  over  drumsep  I  think”  
-
 
Dry
 
Paint
 
Dealer
 
Undr]
   -  If  you  got  an  error  while  using  jarredou’s  Drumsep  Colab  (object  is  not  subscriptable):  change  to  this  on  line  144  in  inference.py:      if  type(args.device_ids)  !=  int:        model  =  nn.DataParallel(model,  device_ids  =  args.device_ids)  (thx  DJ  NUO)   It  works  in  UVR  too.  All  models  should  be  located  in  the  following  folder:  Ultimate  Vocal  Remover\models\MDX_Net_Models   Don't  forget  about  copying  the  config  file  to:  model_data\mdx_c_configs.  Once  the  model  is  detected,  select  the  config  in  a  new  window,  and  that’s  all.   The  model  achieved  much  better  SDR  on  private  jarredou's  small  evaluation  dataset  
compared
 
to
 
the
 
previous
 
drumsep
 
model
 
by
 
Inagoy
 
which
 
was
 
based
 
on
 
a
 
worse
 
dataset
 
and
 
older
 
Demucs
 
3
 
arch.
  The  dataset  for  further  training  is  available  in  the  drums  section  of  Repository  of  stems/multitracks  -  you  can  potentially  clean  it  further  and/or  expand  the  dataset  so  the  results  might  be  better  after  resuming  the  training  from  checkpoint.  Using  the  current  
dataset,
 
the
 
SDR
 
might
 
stall
 
for
 
quite
 
some
 
amount
 
of
 
epochs
 
or
 
even
 
decrease,
 
but
 
it
 
usually
 
increases
 
later,
 
so
 
potentially
 
training
 
it
 
further
 
to
 
300-500-1000
 
epochs
 
might
 
be
 
beneficial.
 Attached  config  also  includes  necessary  training  parameters  for  training  further  using  ZFTurbo  repo.   Current  model  metrics  (not  MVSEP  evaluation  dataset):   “Instr  SDR  kick:  18.4312  Instr  SDR  snare:  13.6083  Instr  SDR  toms:  13.2693  Instr  SDR  hh:  6.6887  
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  