[page 561/642]
    A:  Few  thousand  -  Unwa.  
Mesk
 
for
 
metal
 
dataset
 
at
 
some
 
point
 
had
 
2135
 
instrumentals
 
and
 
1779
 
vocals
 
(total
 
3914
 
tracks)
  For  fine-tuning  of  existing  models  of  these  archs,  RTX  3070  Ti  and  4060  (both  8GB)  were  
used
 
by
 
unwa
 
and
 
Gabox
 
respectively
 
(RTX
 
2000
 
series
 
don’t
 
support
 
flash
 
attention
 implementation  in  ZFTurbo’s  training  repo).   “You  COULD  train  using  8GB  of  VRAM,  it  is  doable,  but  not  recommended,  you  at  least  need  
16
 
or
 
more.
 
Training
 
is
 
difficult
 
because
 
it
 
quickly
 
fills
 
up
 
your
 
VRAM
 
even
 
with
 
gradient
 
checkpointing
 
enabled”
 
-
 
mesk
  Training  was  also  tested  by  unwa  and  working  on  RX  7900  XTX  24GB  (gfx1100)  on  Ubuntu  
24.04
 
LTS
 
using
 
Pytorch
 
2.6
 
for
 
ROCm
 
6.3.3,
 
PyTorch
 
2.6
 
for
 
ROCm
 
6.2.4.
 “No  special  editing  of  the  code  was  necessary.  All  we  had  to  do  was  install  a  
ROCm-compatible
 
version
 
of
 
the
 
OS,
 
install
 
the
 
AMD
 
driver,
 
create
 
a
 
venv,
 
and
 
install
 
ROCm-compatible
 
PyTorch,
 
Torchaudio,
 
and
 
other
 
dependencies
 
on
 
it.”
 “To  install  only  the  minimum  necessary  items,  I  first  installed  PyTorch,  then  ran  train.py  many  
times
 
to
 
install
 
the
 
missing
 
items
 
little
 
by
 
little.”
  “Basically,  it  is  almost  no  different  from  PyTorch  for  CUDA.  For  example,  when  specifying  a  device  in  your  code,  you  can  just  use  'cuda'  as  is.  Also,  Flash  Attention  can  be  used  by  setting  the  environment  variable  to  
‘TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1’.”
  For  now,  the  only  supported consumer  AMD  Radeon  GPUs  for  ROCm  on  Linux  are:   RX  7900  XTX,  RX  7900  XT,  RX  7900  GRE  and  AMD  Radeon  VII  (probably  a  fuller  list  of  GPUs  from  here should  have  working  GPUs  with  ROCm  too),   but  “even  right  now  hipcc  in  ROCm  6.3.3  has  gfx1200  and  gfx1201  targets  [namely  RX  9070  
and
 
RX
 
9070
 
XT].
 
You'll
 
still
 
be
 
able
 
to
 
build
 
and
 
run
 
stuff
 
with
 
ROCm.
 
For
 
whatever
 
reason,
 
AMD
 
feels
 
it's
 
not
 
ready
 
to
 
give
 
its
 
stamp
 
of
 
approval.”
 
(EmergencyCucumber905)
 
E.g.
 
RX
 
6700
 
XT
 
12GB
 
seems
 
to
 
work
 
with
 
ROCm
 
too,
 
but
 
its
 
performance
 
might
 
turn
 
out
 
to
 
be
 
not
 
good
 
enough,
 
seeing
 
how
 
ZLUDA
 
based
 
on
 
ROCm
 
performed
 
(more
 
on
 
ZLUDA
 
later
 
below,
 
it’s
 
rather
 
not
 
feasible
 
for
 
training
 
even
 
in
 
its
 
fork
 
state).
 
  “The  7900  XTX  is  great  but  probably  no  match  for  the  9070  XT,  which  will  be  optimized  in  
time;
 
the
 
7900
 
XTX
 
certainly
 
has
 
more
 
VRAM,
 
but
 
RDNA4
 
has
 
FP8
 
support
 
and
 
greatly
 
enhanced
 
performance
 
at
 
FP16/BF16.
 Also,  the  7900  XTX  is  a  top-end  GPU  and  generates  tremendous  heat.  The  room  becomes  unbearably  hot  after  running  it  for  a  while.”  Unwa   Unwa  half  a  year  later:  “If  you  want  to  use  AI  properly  with  an  AMD  GPU,  the  MI300X  is  the  best  choice.”  
“Honestly,
 
I
 
miss
 
how
 
comfortable
 
CUDA
 
is.”
  
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  