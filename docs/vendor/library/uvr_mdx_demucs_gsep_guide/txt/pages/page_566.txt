[page 566/642]
    oh  and  then  further  split  into  60  second  chunks  after  mixing  them”  -  jowoon   “Aligned  dataset  is  not  a  requirement  to  get  performing  models,  so  you  can  create  a  dataset  
with
 
FL/Ableton
 
with
 
random
 
beats
 
for
 
each
 
stem.
 
Or
 
using
 
loops
 
(while
 
they
 
contain
 
only
 
1
 
type
 
of
 
sound).
 You  create  some  tracks  with  only  kick,  some  others  with  only  snare,  other  with  only...etc...   And  you  have  your  training  dataset  to  use  with  random  mixing  dataloader  (dataset  type  2  in  
ZFTurbo
 
script,
 
one
 
folder
 
with
 
all
 
kick
 
tracks,
 
one
 
folder
 
with
 
all
 
snare
 
tracks,
 
one
 
folder
 
with…
 
etc.
  Then  you  have  to  create  a  validation  dataset  accordingly  to  the  type  of  stems  used  in  
training,
 
preferably
 
with
 
a
 
kind
 
of
 
music
 
close
 
to
 
the
 
kind
 
you
 
want
 
to
 
separate,
 
or
 
"widespread",
 
with
 
a
 
more
 
general
 
representation
 
of
 
current
 
music,
 
but
 
this
 
mean
 
it
 
has
 
to
 
be
 
way
 
larger.
  The  only  requirements  are:  44.1Hz  stereo  audio.  Lossless  (wav/flac)  Only  1  type  of  sound  by  file  (and  no  bleed  like  it  would  happen  with  real  drums)  Audio  length  longer  than  30s  (current  algos  use  mostly  ~6/12  second  chunks,  but  better  to  
have
 
some
 
margin
 
and
 
longer
 
tracks
 
so
 
they
 
can
 
be
 
used
 
in
 
future
 
when
 
longer
 
chunks
 
can
 
be
 
handled
 
by
 
archs
 
&
 
hardware).”
 
jarredou
 “You  can  use  flac  too;  saves  space  (though  make  them  44.1  /  16-bit  /  stereo,  even  if  u  use  
mp3's
 
or
 
whatever
 
other
 
format
 
-
 
convert
 
upfront)
 validation  set  however  needs  to  remain  in  wav  with  mixture  included.”  Bas  Curtiz   “A  quite  unknown  Reaper  script  to  randomize  any  automatable  parameters  on  any  
VST/JS/ReaXXX
 
plugin
 
with
 
MIDI
 
notes.
 
It's
 
REALLY
 
a
 
must-have
 
for
 
dataset
 
creation,
 
adding
 
sound
 
diversity
 
without
 
hassle.
 https://forum.cockos.com/showthread.php?t=234194”  jarredou   (Guides  for  stem  limiting  moved  to  the  end  of  the  section  for  archival  purposes  -  rather  
outdated
 
approaches
 
due
 
to
 
the
 
statements
 
in
 
the
 
paper
 
above)
   FAQ   You  shouldn't  compare  training  data  against  evaluation  data,  while  those  being  the  same.   You  can  use  multisong  dataset  from  MVSEP,  and  make  sure  you  don't  have  any  of  those  
songs
 
in
 
your
 
dataset.
  Q:  Does  evaluation  data  matter  for  the  final  quality  of  the  model?  
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  