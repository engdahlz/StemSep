[page 577/642]
    evaluation  and  the  result  was  only  0.31  higher  than  the  best  current  ensemble  (although  it  
used
 
lower
 
parameters
 
for
 
separation).
 
Generally,
 
to
 
break
 
through
 
that
 
wall,
 
we
 
may
 
need
 
to
 
utilize
 
multi-GPU
 
with
 
batch
 
size
 
“16
 
or
 
even
 
8”.
  “If  you  did  this  experiment  with  batch  size  16  or  even  8  you  would  see  much  better  
performance
 
I
 
think”
 
-
 
Kim
 “mhm  but  that  requires  multi  GPU”  -  Bas  “yeah  that  is  the  wall  I  think”  -  Kim   -  “at  least  for  vocals,  using  the  default  8192  n_fft  for  mdx23c  and  reducing  hop_length  from  
2048
 
to
 
1024
 
gave
 
better
 
results
 
(it's
 
InstVocHQ
 
config
 
iirc).
 In  mdx23c  paper,  they  got  better  score  with  higher  n_fft/hop_length  resolution.   
Hop_length
 
means
 
the
 
portion
 
of
 
audio
 
that
 
will
 
NOT
 
be
 
overlapped
 
during
 
STFT
 
processing.
 
So
 
the
 
lower,
 
the
 
higher
 
overlap
 
you
 
get
 
in
 
the
 
end.
 
And
 
a
 
bit
 
like
 
the
 
overlap
 
we
 
are
 
doing
 
during
 
inference
 
(which
 
is
 
different,
 
as
 
applied
 
on
 
waveform,
 
on
 
way
 
larger
 
scale)
 
but
 
in
 
the
 
same
 
way,
 
the
 
higher
 
overlap
 
you
 
use,
 
the
 
higher
 
quality
 
you
 
get,
 
but
 
at
 
cost
 
of
 
more
 
resources,
 
and
 
at
 
some
 
point
 
it
 
gets
 
stagnant
 
(or
 
could
 
even
 
reduce
 
quality
 
too
 
if
 
set
 
too
 
high)”
 
-
 
jarredou
  -  For  “ringing  and  unpleasant  artifacts  at  subband  edges,  (...)  more  of  a  dip,  that  seems  to  
get
 
filled
 
progressively
 
along
 
training”
 >  “It  seems  like  overlapped  subbands  is  the  thing  (not  surprised  about  it)  but  the  dips  are  
maybe
 
a
 
bug,
 
I'll
 
see
 
if
 
I
 
can
 
improve
 
that.
 I'm  still  experimenting  to  find  best  way  to  alleviate  the  subband  artifacts  of  mdx23c,  but  
here's
 
an
 
already
 
working
 
fork
 
with
 
separatable
 
depthwise
 
convs
 
(model
 
size
 
is
 
divided
 
by
 
±6)
 DL  -  “[The]  opposite  to  BS/Mel  Rofos,  the  more  you  have  sub-bands  with  mdx23c  the  less  
resources
 
needed
 
to
 
train
 
a
 
model
 
(until
 
quick
 
collapse
 
of
 
the
 
model
 
if
 
too
 
much
 
sub-bands
 
used,
 
from
 
what
 
I've
 
experimented
 
until
 
now)”
 
-
 
-||-
 Frazer:  “wouldn't  another  fix  be  to  change  how  the  transposed  convolutions  work?  So  that  
the
 
input
 
to
 
the
 
TConv
 
includes
 
the
 
other
 
bands
 
first
 
or
 
last
 
N
 
indices
 then  crop  it  out  afterward  or  sum/avg  those  extra  indices   surely  if  what's  happening  is  that  as  the  bands  are  downsampled  either  they  drop  indices  at  
the
 
edges
 
because
 
it's
 
not
 
a
 
perfect
 
crop
 
or
 
that
 
for
 
whatever
 
reason
 
the
 
latents
 
at
 
the
 
edges
 
don't
 
receive
 
proper
 
gradients
 
and
 
don't
 
optimize
 
-
 
then
 
accounting
 
for
 
that
 
by
 
either
 
expanding
 
input
 
bands
 
or
 
allowing
 
the
 
convs
 
to
 
work
 
on
 
the
 
borders
 
maybe
 
fix
 
it.
 
I
 
like
 
your
 
idea
 
more,
 
it's
 
cleaner,
 
but
 
I'm
 
just
 
trying
 
to
 
come
 
up
 
with
 
some
 
system
 
where
 
it's
 
cheaper
 
than
 
just
 
having
 
bigger
 
bands,
 
yk.
 
I
 
mean,
 
your
 
system
 
would
 
fix
 
it,
 
but
 
I
 
think
 
you'd
 
need
 
to
 
probably
 
drop
 
some
 
indices
 
right
 
near
 
the
 
border
 
on
 
the
 
expanded
 
bands
 
since
 
those
 
would
 be  broken  in  the  same  way  as  they  are  currently,  if  that  makes  sense”  artefacts [the  modified  code]  Confirmed  with  vanilla  OG  mdx23c  code,  with  only  that  change  and  
nothing
 
else.”
 Q:  How  about  changing  it  to  a  depthwise  separable  convolution  +  pointwise  convolution?  
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  