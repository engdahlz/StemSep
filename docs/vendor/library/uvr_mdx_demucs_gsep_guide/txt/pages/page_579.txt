[page 579/642]
    A:  I've  read  that  it's  better  to  use  a  multiple  of  the  number  of  tracks  in  the  dataset  (here  317  *  
4)
 
To
 
avoid
 
that
 
some
 
of
 
the
 
tracks
 
are
 
used
 
more
 
than
 
others
 
at
 
each
 
epoch.
 
(jarredou)
  jarredou/Aufr33  drumsep  model  was  trained  with  3  seconds  chunks.   -  Here  you  can  find  Colab made  by  yukunelatyh  (dead)   -  Colab by  jazzpear96  (with  the  OG  MSST  repo;  maybe  you  could  just  replace  the  link)   I  think  later  I  quote  jarredou  on  training  with  extremely  low  parameters  configs  on  other  archs  
as
 
well.
  -  Here the  community  guide  one  user  on  training  on  3060  12GB  (invite)  ____  JFI  -  Multi  Source  Diffusion   https://github.com/gladia-research-group/multi-source-diffusion-models Some  results  posted  by  Bytadance  were  labelled  as  “MSS”  but  it’s  rather  not  the  same  arch.  In  the  original  MSS  
paper
 
above,
 
only
 
Slakh2100
 
was
 
used.
 
 ByteDance  probably  expanded  it  further,  and  had  it  was  said  they  had  issues  with  their  legal  department  with  
making
 
their
 
work
 
public,
 
so
 
they
 
can
 
equally
 
use
 
unauthorized
 
stems
 
just
 
like
 
us,
 
or
 
looking
 
for
 
ways
 
to
 
monetize
 
their
 
new
 
discovery
 
for
 
TikTok,
 
as
 
their
 
company
 
largely
 
invest
 
in
 
GPUs
 
lately,
 
so
 
something
 
might
 
happen
 
maybe
 
in
 
the
 
end
 
of
 
the
 
year,
 
and
 
maybe
 
it
 
will
 
be
 
released
 
in
 
their
 
exclusive
 
service
 
(Ripple
 
and
 
Capcut
 
were
 
released
 
later
 
indeed).
 
TBH,
 
it's
 
hard
 
to
 
get
 
a
 
good
 
model
 
using
 
only
 
public
 
datasets.
 
For
 
public
 
archs,
 
it's
 
even
 
impossible.
 
They
 
probably
 
know
 
it
 
too,
 
so
 
it’s
 
kinda
 
grey
 
zone,
 
sadly,
 
and
 
model
 
trained
 
later
 
for
 
Ripple
 
was
 
probably
 
done
 
from
 
scratch
 
and
 
contains
 
only
 
lossy
 
files
 
for
 
training
 
from
 
now
 
on.
 
 Bytedance  (Ripple  too?)  was  said  to  train  on  500  songs  only  +  MUSDBHQ     BS-Roformer  (one  of)  The  best,  but  the  slowest  tested  arch  out  of  the  all  in  this  doc  SDR-wise.  Once  
considered
 
as
 
SOTA
 
(state-of-the-art
 
algorithm),
 
but
 
it
 
has
 
its
 
own
 
caveats,
 
like
 
very
 
strong
 
denoising
 
(which
 
is
 
double-edged
 
sword
 
and
 
might
 
give
 
too
 
muddy
 
results
 
frequently),
 
but
 
using
 
Mel-Roformer
 
and
 
proper
 
config
 
tweaks
 
and
 
prioritizing
 
stem
 
there
 
helped
 
for
 
the
 
muddiness
 
issues.
 
Also,
 
Mel-Roformer
 
has
 
a
 
bigger
 
SDR
 
according
 
to
 
the
 
Mel
 
paper
 
(with
 
an
 
exception
 
for
 
bass),
 
and
 
it’s
 
better
 
for
 
vocals
 
than
 
BS.
 
Plus,
 
Mel
 
seems
 
to
 
handle
 
creating
 
duet
 
singing
 
separation
 
model
 
better.
 
“BS
 
(...)
 
uses
 
more
 
VRAM;
 
unlike
 
Mel,
 
BS
 
has
 
no
 
overlap
 
between
 
bands,
 
so
 
VRAM
 
usage
 
and
 
model
 
size
 
are
 
smaller.”
 
Unwa
 
More
 
below.
 
 
“I
 
find
 
it
 
cute
 
how
 
they
 
call
 
the
 
Transformer
 
based
 
models
 
(which
 
destroy
 
the
 
older
 
convnets)
 
"Roformers"
 
because
 
they
 
use
 
RoPE
 
embeddings.
 
By
 
that
 
naming
 
scheme,
 
all
 
llama-like
 
models
 
are
 
Roformers
 
too...”
 
kalomaze
 
“By
 
the
 
way,
 
it
 
wasn't
 
us
 
who
 
started
 
calling
 
Transformer
 
using
 
RoPE
 
"Roformer.
 https://arxiv.org/abs/2104.09864 
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  