[page 631/642]
     I've  seen  that  paper  in  my  feed  last  month,  doing  real-time  source  separation  (23ms  latency):  https://arxiv.org/abs/2402.17701  Mamba:  Linear-Time  Sequence  Modeling  with  Selective  State  Spaces  https://www.youtube.com/watch?v=9dSkvxS2EB0   Vocal  restoration  research  https://github.com/facebookresearch/AudioMAE https://carlosholivan.github.io/demos/audio-restoration-2023.html https://google.github.io/df-conformer/miipher/ https://arxiv.org/abs/2403.05393 https://github.com/vikastokala/bcctn https://github.com/espnet/espnet  https://github.com/manosplitsis/hifi-gan-bwe/tree/train_with_music “new  vocoder  replacing  hifi-gan,  vocos,  bigvgan  etc  compared  to  other  ones,  high  freq  smearing  practically  doesn't  occur”  EVA-GAN  -  another  breakthrough  over  HiFi-GAN  https://arxiv.org/abs/2402.00892 https://arxiv.org/pdf/2402.00892.pdf  What  can  potentially  help  on  training  on  inferior  GPUS  with  large  model  size  is  LORA.   "especially  since  bsrofromer  is  transformer  then  you  can  allow  users  to  finetune  even  the  largest  loras  work  by  going  through  a  model  and  replacing  all  the  linear  projections  with  a  wider  
projection
 
(then
 
i
 
think
 
projecting
 
back
 
to
 
the
 
original
 
size)
 so  imagine  you  got  a  linear  projection  thats  trained,  ie  its  4  neurons  in  4  neurons  out  -  lora  
works
 
by
 
adding
 
X
 
neurons
 
either
 
side
 
of
 
the
 
projection
 
(i
 
cant
 
really
 
remember
 
but
 
its
 
somethign
 
like
 
this)
 
 then  you  freeze  all  the  other  stuff  around  the  linears  and  only  train  the  new  linears  (the  lora)  if  you  open  up  the  lora  source  code  you'll  see  what  i  mean,  theres  a  loop  that  just  iterates  all  
the
 
weights
 
and
 
replaces
 
the
 
linears
 
with
 
a
 
loralinear
 
(and
 
thats
 
the
 
entire
 
method)
 lora  will  allow  for  better  SDR  on  whatever  you  trained  (albeit  a  small  sample  set)  so  itd  be  super  smart  to  treat  it  like  how  image  gen  treats  it,  so  u  can  say  make  a  lora  for  
crowd
 
removal
 
mixed
 
with
 
the
 
lora
 
for
 
vocals
 then  ppl  in  this  disc  can  try  create  the  best  lora  for  their  specific  usecase  and  mix  match  with  
other's"
 
frazer
 More:  https://radekosmulski.com/how-to-fine-tune-a-tranformer-pt-2/   "I  think  here  it  can  be  useful  if  we  will  have  very  great  multistem  model  and  after  finetune  it  
on
 
rare
 
instruments."
 
ZFTurbo
 
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  