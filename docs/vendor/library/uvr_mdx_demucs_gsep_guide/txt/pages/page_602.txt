[page 602/642]
    Anv:  there's  jax  version  https://github.com/flyingblackshark/jax-bs-roformer  Fr:  You  dont  need  JAX  -  all  you  need  to  do  is  import  torchxla  and  move  it  to  xla  device   wherever  the  model  is  instantiated,  I  think  train.py  probs,  add  that  import  at  the  top,  where  
the
 
model
 
is
 
moved
 
to
 
device
 
-
 
change
 
that
 
from
 
.to('cuda')
 
to
 
.to('xla'),
 
then
 
in
 
the
 
train
 
loop
 
you
 
add
 
the
 
with
 
torch_xla
 
step,
 
move
 
inputs
 
to
 
xla
 
then
 
after
 
that
 
train
 
loop
 
you
 
put
 
xla
 
sync
   Bas  stuck  during  fixing  it  before,  but  here’s  the  convo  where  he  stuck:  ________________________________   Why  does  training  the  bs_roformer  model  with  8s  chunksize,  256  dim,  8  depth  consume  only  
13GB
 
of
 
VRAM
 
now,
 
compared
 
to
 
21GB
 
last
 
time
 
[they
 
could
 
decrease
 
VRAM
 
since
 
then]
  Stuck  troubleshooting  of  TPU  training  by  Bas  Curtiz  (Q)  and  frazer,  DJ  NUO,  jarredou  and  
Cyclcrclicly
 
(A):
  Q:  is  it  as  simple  as  adding  pytorch  lightning  though?  A:  try  using  "xla"  as  the  device  instead  of  cuda  and  if  you're  lucky  everything  will  Just  Work ™    Q:  The  Pytorch's  implementation  of  stft  does  not  work  on  the  XLA  (TPU),  because  it  internally  uses  some  unsupported  functions.  There  are  not  feasible  workarounds  for  it  available.  Only  some  3x  PhD  discussion,  which  discusses  the  underlying  function  not  working,  which  would  require  forking  pytorch  to  get  it  working,  IF  the  solution  was  actually  even  
feasible:
 
 (hacky  super  slow  workaround,  or  just  "use  different  shit").   Only  "realistic"  solution  I've  found  is  porting  the  mel  band  roformer  to  tensorflow.   Which  is  bruh,  but  the  thing  is  in  their  docs  STFT  says:   Implemented  with  TPU/GPU-compatible  ops  and  supports  gradients..   Also  tensorflow  is  by  google,  the  TPU  as  well,  so  yk,  it  might  have  better  support.   The  same  error  basically  is  described  here:  https://github.com/pytorch/xla/issues/2241  A:  As  frazer  said,  you'll  have  better  luck  with  jax  than  tensorflow  A:  Can  you  try  putting  data  to  CPU  &  running  it  there,  and  then  put  the  result  back  on  TPU?   I  encounter  similar  issues  when  running  on  Mac  MPS  (GPU),  and  this  code  helps  to  alleviate  
the
 
issue:
 stft_repr  =  torch.stft(raw_audio.cpu()  if  x_is_mps  else  raw_audio,  **self.stft_kwargs,  
window=stft_window.cpu()
 
if
 
x_is_mps
 
else
 
stft_window,
 
return_complex=True).to(device)
  
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  