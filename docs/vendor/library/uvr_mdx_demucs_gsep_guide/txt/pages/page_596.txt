[page 596/642]
    -  optional  but  recommended-   Create  a  free  account  at  https://wandb.ai -  shows  u  more  insight  on  the  training  progress  with  graphs.  A  free  personal/cloud-hosted  account  should  suffice.  Add  parameter  --wandb_key  YOUR_API_KEY  (which  u  can  get  from  
https://wandb.ai/authorize)
  Save  the  full  command  in  a  text-file,  handy  for  future  usage.  Hereby  mine,  which  u  can  alter:   python  train.py  --model_type  mel_band_roformer  --config_path  
configs/config_musdb18_mel_band_roformer_bascurtiz.yaml
 
--dataset_type
 
2
 
--results_path
 
results
 
--data_path
 
datasets/train
 
--valid_path
 
datasets/validation
 
--num_workers
 
4
 
--device_ids
 
0
 
--wandb_key
 
e304f2CENSOREDSOYOUNEEDTOUSEYOUROWNdecc122e
  Run  the  command  in  the  root  folder  with  CMD.  Check  your  progress/graphs  at  https://wandb.ai/[yourusername]/projects   Latest  update  of  repo  gives  u  insight  into  fullness/bleedless  too  using  parameter:   --metrics  sdr  bleedless  fullness  l1_freq  si_sdr  log_wmse  aura_stft  aura_mrstft”  
 “the  augmentations  help  tho  even  tho  it's  slower  gives  it  much  more  situations  for  real  songs”  “ye  if  your  dataset  isn't  from  the  biggest  it  will  help”  
 
“If
 
you
 
plan
 
to
 
stop/resume
 
training
 
many
 
times,
 
it
 
could
 
be
 
interesting
 
to
 
also
 
save
 
optimizer
 
(and
 
scheduler)
 
state
 
with
 
checkpoint,
 
it
 
can
 
help
 
train
 
faster
 
when
 
you
 
stop/resume
 
a
 
lot
 
(as
 
you
 
resume
 
everything
 
in
 
the
 
state
 
it
 
was
 
when
 
you
 
stopped
 
training,
 
instead
 
of
 
restarting
 
optimizer
 
from
 
scratch
 
each
 
time).”
  Patience  parameter   “If  set  too  low,  it  will  reduce  learning  rate  way  too  fast  and  lead  to  stagnant  learning”.  So,  if  "the  model  did  not  improve  for  like  10  epoches  (weights  did  not  save)"  while  patience  
is
 
set
 
to
 
default
 
2,
 
"you
 
should
 
set
 
it
 
to
 
like
 
1000
 
to
 
disable
 
it
 
for
 
now."
 "patience  =  X  means  that  if  during  training,  X  consecutive  epochs  are  not  giving  
improvement
 
(using
 
sdr
 
metric
 
by
 
default),
 
it
 
will
 
reduce
 
learning
 
rate.
 
If
 
not
 
set
 
correctly,
 
it
 
can
 
kill
 
a
 
training
 
run
 
by
 
reducing
 
too
 
fast
 
and
 
too
 
early
 
learning
 
rate.
  When  not  sure,  it's  better  to  set  it  to  really  high  value  (like  1000  here)  so  it  will  be  never  
triggered."
 
-
 
jarredou
  Made  from  scratch  training  script  by  Dill  “https://github.com/dillfrescott/mvsep-beta “it  uses  a  neural  operator  architecture  with  something  I  call  Kernel  Scale  Attention  to  capture  
a
 
range
 
of
 
details.
 
I'm
 
training
 
it
 
now.
 
No
 
guarantees
 
tho
 
on
 
the
 
quality
 
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  