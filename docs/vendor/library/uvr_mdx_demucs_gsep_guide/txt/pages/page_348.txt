[page 348/642]
    newer  modded  Radeon-ID  drivers  and/or  with  downgraded  DirectML.dll  attached  with  your  
drivers,
 
copied
 
to
 
UVR\torch_directml
 
folder)
 Intel  was  confirmed  to  work  with  ARC  GPUs,  and  Xe  integrated  graphics  (e.g.  Tiger  Lake  
2021)
 
with
 
MDX-Net
 
HQ
 
(v2)
 
models.
 RTX  5090  is  not  yet  supported  in  official  UVR  packages,  you  can  use  OpenCL  (DirectML)  in  
options
 
instead
 
(slower).
 
 2GB  cards  will  probably  cause  issues,  and  4GB  VRAM  too  -  at  least  on  certain  Roformer  
models
 
and
 
settings
 
(unless
 
dim_t
 
equivalent
 
of
 
chunk_size
 
is
 
set
 
to
 
301/201,
 
but
 
201
 
might
 have  a  bit  of  audible  audio  skipping  at  times  -  see  here later  below  for  dim_t>chunks_size  conversion)     -  Not  meeting  these  requirements,  youâ€™re  forced  using  CPU  processing,  which  is  very  slow  -  
even
 
Ryzen
 
5950X
 
is
 
slower
 
than
 
1050
 
Ti
 
in
 
this
 
application,
 
and
 
1700X
 
is
 
slower
 
by
 
double
 
than
 
even
 
940M.
 
  -  Since  the  last  updates  you  can  also  use  AMD/Intel  GPU,  with  separate  installer  with  
OpenCL
 
support
 
(most
 
likely
 
min.
 
requirement
 
is
 
GCN
 
or
 
Polaris
 
architectures
 
and
 
up
 
-
 
HD
 
7XXX
 
and
 
RX
 
4XX,
 
but
 
even
 
4GB
 
variants
 
may
 
crash
 
on
 
certain
 
settings).
 
 
 
-
 
Old
 
drivers
 
for
 
GCN
 
GPUs
 
might
 
fail
 
using
 
GPU
 
Conversion
 
option.
 
Consider
 
using
 
Radeon.ID
 
drivers
 
or
 
using
 
DirectML.dll
 
copied
 
from
 
your
 
Windows
 
installation
 
into
 
Ultimate
 
Vocal
 
Remover\torch_directml
  -  You  can  also  use  Mac  M1  for  GPU  acceleration  (MPS  GPU  support  in  separate  installer),  
but
 
also
 
Radeons
 
acceleration
 
on
 
Intel
 
Macs
 
is
 
supported
  -  GTX  1660  Ti  (6GB)  is  slow  for  separation  using  Roformer  models.  A  better  choice  is  RTX  
3050
 
despite
 
similar
 
performance
 
in
 
games,
 
3050
 
has
 
the
 
same
 
amount
 
of
 
CUDA
 
cores
 
as
 
Tesla
 
T4
 
on
 
Colab,
 
but
 
with
 
less
 
VRAM.
 
Avoid
 
the
 
laptop
 
variant
 
of
 
3050
 
which
 
has
 
only
 
4GB
 
of
 
VRAM,
 
and
 
not
 
8GB
 
like
 
the
 
desktop
 
variant.
  -  If  you  want  a  fast  2nd  hand  GPU  with  more  VRAM,  consider  1080  Ti  or  2080  Ti  or  even  
3080
 
Ti
 
(16GB).
 
Pretty
 
fast
 
ones
 
for
 
separations.
 1080  Ti  is  much  faster  in  this  task  than  3060  12GB.  https://media.discordapp.net/attachments/767947630403387393/1133164474749169864/image.png (dead)   Q:  My  AMD/Intel  GPU  have  sudden  spikes  of  usage,  or  just  30%  is  being  utilized.  Is  that  a  
CPU
 
bottleneck?
  A:  Nope.  It's  just  how  inefficiently  DirectML  behaves.  It's  normal  and  happens  for  all  people,  
even
 
on
 
some
 
ancient
 
4
 
cores.
 
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  