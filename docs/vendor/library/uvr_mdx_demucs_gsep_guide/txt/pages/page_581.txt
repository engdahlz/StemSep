[page 581/642]
     Kim’s  Mel  training  config made  for  H100  (model  on  x-minus  was  trained  for  3  weeks)  which  viperx  probably  used  later  for  drums  model  or  reworked  for  his  GPU.  Kims  says  5.0e-05  is  already  low  enough  learning  rate,  setting  it  too  low  may  make  it  too  
slow
 
to
 
train.
 
Kim
 
“also
 
said
 
that
 
during
 
the
 
end
 
of
 
her
 
training
 
the
 
loss
 
would
 
plateau
 
but
 
the
 
SDR
 
was
 
improving”,
 
“no
 
patience,
 
no
 
LRreduceonpleateau
 
at
 
all.
 
While,
 
if
 
set
 
incorrectly,
 
it
 
can
 
ruin
 
your
 
training
 
very
 
quickly”
 
-
 
jarredou
  With  the  unwa’s  inst  v1  model  it  turned  out,  prioritizing  stem  in  the  config  matters  a  lot,  so  
depending
 
on
 
which
 
model
 
you
 
want
 
(other,
 
instrumental,
 
null,
 
multistem
 
like
 
in
 
duality
 
models
 
or
 
vocal)
 
that’s
 
what
 
you
 
should
 
set
 
in
 
the
 
config.
 
Although,
 
prioritizing
 
stem
 
to
 
instrumental
 
gave
 
a
 
noise
 
similar
 
to
 
VR
 
or
 
MDX-Net
 
v2
 
models
 
(but
 
not
 
the
 
same).
 
We
 ended  up  with  the  code based  on  Aufr33  idea,  recreated  by  Becruily,  that  copies  phase  from  Mel-Kim  model  which  is  deprived  of  the  noise  and  doesn’t  prioritize  vocal  stem  like  unwa  inst  
1/1e/v2
 
models,
 
and
 
it
 
gets
 
rid
 
of
 
some
 
noise
 
in
 
those
 
models.
 
Aufr33
 
own
 
implementation
 
is
 
added
 
in
 
ensemble
 
on
 
x-minus.pro/uvronline
 
and
 
in
 
UVR
 
latest
 
patches
 
there’s
 
becruily
 script rewrite.   According  to  mesk,  fine-tuning  a  fine-tuned  model  might  be  a  worse  solution  than  simply  
fine-tuning
 
the
 
Kim's
 
model
 
(from
 
experience
 
on
 
training
 
on
 
genre-oriented
 
dataset
 
like
 
metal
 
which
 
Mesk
 
tried
 
to
 
train).
  “I  got  an  error  when  I  set  num_stems  to  2.”  unwa  
You
 
can
 
use
 
“target_instrument:
 
null”
 
instead,
 
which
 
is
 
also
 
required
 
for
 
multistem
 
training
 like  on  this example  ~jarredou   “increasing  num_stems  increases  model  size”  “multistem  is  like  having  multiple  checkpoints  
in
 
one
 
file
 
(1
 
for
 
each
 
stem).
 
All
 
model
 
types
 
work
 
like
 
that
 
with
 
ZFTurbo's
 
script
 
AFAIK”
 
  use_torch_checkpoint:  true  in  the  current  MSST  repo  will  reduce  VRAM  usage.   Using  various  chunk_size  during  different  stages  of  the  training  can  be  helpful,  and  also  
using
 
different
 
dataset
 
sizes
 
based
 
(e.g.
 
leaving
 
only
 
more
 
clean
 
or
 
official
 
at
 
certain
 
points).
  
___
  First  BS-Roformer  models  were  trained  on  ZFTurbo  dataset,  later  viperx  trained  on  his  own,  
presumably
 
larger
 
dataset
 
(and
 
possibly
 
better
 
GPU)
 
and
 
achieved
 
better
 
SDR,
 
then
 
another
 
model
 
was
 
made
 
from
 
fine-tuning
 
viperx
 
model
 
on
 
ZFTurbo
 
dataset,
 
and
 
Kim’s
 
Mel
 
model
 
was
 
trained
 
on
 
Aufr33
 
dataset
 
from
 
UVR,
 
later
 
Unwa
 
trained
 
on
 
Bas
 
Curtiz’
 
dataset
 
(?too).
  You  can  use  ZFTurbo  code  as  base  for  training  Roformers:  https://github.com/ZFTurbo/Music-Source-Separation-Training 
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  