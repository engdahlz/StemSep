[page 593/642]
            time  =  self.embedding_time(time).view(B,  1,  -1)                   conditioning  =  time  #can  add  in  other  embeddings  here          x  =  self.forward_model(x,  conditioning)          return  x  if  u  want  different  losses  you  have  to  adapt  them,  what  ur  trying  to  do  is  to  iteratively  change  
the
 
input
 
by
 
adding
 
something
 
to
 
the
 
latent
 
across
 
N
 
timesteps”
  Q:  Is  there  a  version  of  Mel-Roformer  without  all  the  nonsense  value  residuals  and  stuff  like  
that
 A:  Yeah  zfturbo  separated  those  a  few  weeks  ago  on  a  new  file  “https://github.com/ZFTurbo/Music-Source-Separation-Training/blob/main/models/bs_roformer/mel_band_roformer.py or  I  guess  lucidrains  file  is  as  bare/og  as  it  gets”   Later,  frazer’s  blocks  are  written  somewhere  in  #dev-talk.   -  More  Roformer  training  insides  in  far  right  of  this Bas  Curtiz’  sheet  (phase  image  may  overlap  the  text,  navigate  by  arrows  to  read  it  down  below).   “bleedless/fullness  metrics  are  stft  magnitude-only  based  and  as  they  are  discarding  the  
phase
 
data,
 
they
 
have
 
some
 
kind
 
of
 
blind
 
spots.
 I  guess  this  noise  could  be  also  reduce  by  using  higher  n_fft  values  for  model  (smaller  bins,  
finer
 
freq
 
separations,
 
but
 
way
 
more
 
ressources
 
needed
 
to
 
train
 
models)”
 
-
 
jarredou
 
Q:
 
High
 
n_fft
 
values
 
increase
 
the
 
frequency
 
resolution
 
but
 
decrease
 
the
 
time
 
resolution
 A:  Yeah.  But  Roformers  are  trained  with  2048,  it's  not  high  value.  MDX23C  original  models  
are
 
using
 
8192
 
by
 
default,
 
with
 
improved
 
results
 
compared
 
to
 
lower
 
values.
 
(ZFTurbo
 
has
 
made
 
some
 
tests
 
back
 
then
 
comparing
 
different
 
n_fft/hop_length
 
config)
 
 Q:  does  it  matter  that  much  when  the  model  is  trained  with  multi  resolution  󰤇  it  should  cover  
both
 
low
 
and
 
high
 
nfft
 
values
 A:  n_fft=2048  is  around  21.53  Hz  resolution  per  bin  (on  linear  scale)   while  n_fft=8192  gives  5.39  Hz  resolution  per  bin  This  should  benefit  most  of  the  stems  types  (maybe  not  drums  and  transient  heavy  content  
tho)
 We  don't  have  multi-resolution  arch  yet,  even  if  it  could  be  interesting.  Only  the  loss  in  
multi-resolution,
 
not
 
the
 
model.
 
-
 
jarredou
  Q:  Does  finetuning  need  to  reach  hundreds  of  epochs?  A:  “Not  always.  Depends  on  the  amount  of  data  -  if  it's  small  you  could  literally  get  away  with  
training
 
for
 
a
 
day
 
or
 
half
 
a
 
day”
 
40
 
hours
 
is
 
enough
 
for
 
just
 
fine-tuning
 
(frazer)
  Q:  Is  there  something  wrong  with  the  config  I’m  using?  I’ve  already  tried  training  3  times,  and  
the
 
pattern
 
is
 
always
 
the
 
same,
 
after
 
the
 
training
 
metrics
 
improve
 
about
 
6
 
times,
 
it
 
becomes
 
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  