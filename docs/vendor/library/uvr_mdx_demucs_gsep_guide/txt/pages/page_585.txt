[page 585/642]
    I  think  these  settings  can  give  batch_size  >  4   For  example,  I  can't  finetune  viperx  model  on  my  computer  with  48GB  A6000  because  the  
model
 
is
 
too
 
large.
 chunk_size  is  what  affect  model  size  the  most,  I  think.  And  I  saw  it's  possible  to  get  good  
result
 
with
 
small
 
chunk
 
size.
 I  put  the  table  here:  https://github.com/ZFTurbo/Music-Source-Separation-Training/blob/main/docs/bs_roformer_info.md”   [see  also  https://lambdalabs.com/gpu-benchmarks batch  size  chosen  in  Metric,  fp16,  but  ZFTurbo  said  that  training  on  fp32  is  also  possible]   The  0  SDR  issue  was  later  fixed:  “the  non  A100  issue  is  fixed  with  latest  torch,  but  I  think  the  
general
 
rule
 
is
 
that
 
batch
 
size
 
1
 
(like
 
in
 
my
 
case
 
with
 
3090)
 
won't
 
give
 
good
 
results
 
on
 
Roformers.
 But  I’ve  been  doing  batch  size  2-4  with  A6000  and  A100  and  no  issues  there.  But  the  model  is  also  large,  when  I  finetuned  a  smaller  Mel-Roformer  the  3090  worked  there”   “Full  sized  Roformers  are  very  heavy  to  train  and  unless  you  have  crazy  hardware  like  
A6000
 
(the
 
very
 
minimum),
 
A100
 
etc,
 
training
 
from
 
scratch
 
will
 
take
 
months
 
to
 
get
 
a
 
good
 
model
 
(with
 
SDR
 
similar
 
to
 
current
 
ones,
 
and
 
this
 
is
 
without
 
considering
 
the
 
dataset).
 Maybe  someone  with  4090  can  give  more  insight,  but  I  personally  can't  train/finetune  a  full  
sized
 
Roformer
 
model
 
with
 
my
 
3090,
 
it's
 
way
 
too
 
weak,
 
I'd
 
have
 
to
 
make
 
the
 
model
 
smaller
 
meaning
 
the
 
quality
 
won't
 
be
 
as
 
good
 
as
 
the
 
original
 
checkpoint”
 
becruiily
  Q:  Can  we  change  model  size  of  existing  model  and  fine  tune  it?  Or  it  must  have  been  
trained
 
from
 
scratch
 
with
 
the
 
same
 
chunk
 
size
 
 A:  1)  If  you  just  decrease  chunk  size,  it  will  work  almost  the  same  as  with  larger  (as  I  
remember)
 2)  If  you  decrease  dim  or  depth,  score  will  drop  very  much"   Don't  forget,  each  time  you  change  something  in  dataset,  you  have  to  delete  metadata_x.pkl  
file
 
to
 
create
 
new
 
database
 
on
 
training
 
launch
 
taking
 
into
 
account
 
new
 
changes
 
(it
 
made
 
me
 
become
 
crazy
 
during
 
my
 
first
 
tests
 
when
 
forgetting
 
to
 
delete
 
it)
  I've  just  checked  ZFTurbo's  code,  and  for  dataset  type  2,  the  ".wav"  extension  is  still  required  
for
 
the
 
script
 
to
 
find
 
the
 
files
 
(it
 
doesn't
 
work
 
with
 
any
 
other)
  Q:  It  is  possible  to  finetune  on  3080  Ti  8GB?  unwa  did  it  [on  3070  Ti  8GB]   A:  “By  reducing  the  chunk_size  and  using  AdamW8bit  for  the  optimizer,  I  was  able  to  train  
even
 
with
 
8GB
 
of
 
VRAM.”
 
Usually
 
such
 
fine
 
tune
 
on
 
inferior
 
GPU
 
was
 
decreasing
 
SDR.
 
Here
 
it
 
only
 
dropped
 
by
 
0.1
 
SDR
 
after
 
few
 
thousands
 
steps
 
of
 
training
 
(so
 
only
 
a
 
few
 
epochs)
 
on
 
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  