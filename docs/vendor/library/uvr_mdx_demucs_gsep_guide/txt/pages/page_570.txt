[page 570/642]
    (I  use  sharepoint/onedrive  for  that,  but  u  can  use  buzzheavier.com  for  unlimited  storage)   Done.   “In  some  cases  when  there  aren't  clean  versions  available,  you  can  use  a  portion  of  the  song  
where
 
it
 
doesn't
 
have
 
vocals
 
(but
 
has
 
the
 
bleed
 
instruments)
 
and
 
add
 
random
 
clean
 
vocals
 
 it's  not  aligned  dataset,  but  works  for  fine-tuning”  -  becruily  even  aufr33  admitted  that  makes  models  with  isolated  tracks  
 
“Lossless
 
is
 
always
 
better
 
(and
 
if
 
needed
 
you
 
can
 
use
 
mp3
 
encoding
 
as
 
an
 
augmentation
 
during
 
training,
 
based
 
on
 
the
 
lossless
 
files)
 But  as  320kbps  have  quite  high  cutoff  (20khz  or  something),  it  would  be  less  problematic  
than
 
more
 
compressed
 
audio
 
with
 
hard
 
cutoff
 
at
 
16khz
 
or
 
17khz.
 I  would  say  that,  like  for  other  less  regular  stuff  in  your  dataset,  make  it  obvious  in  filename  
that
 
it's
 
not
 
lossless
 
if
 
you
 
share
 
these
 
files
 
 Q:  So...  maybe  not?  IDK  I  feel  like  I  could  make  an  entire  20  songs  dataset  out  of  those,  
because
 
the
 
best
 
ones
 
aren't
 
lossless
 but  would  it  actually  be  helpful  a  lot  of  leaked  stuff  is  in  320kbps  mp3  A:  I  think  while  codec  cutoff  is  around  20khz  or  above  it's  ok.  Because  that  will  not  bias  model  output  results.   With  first  Roformers  models  from  ByteDance  on  ripple,  that  were  trained  on  mixed  lossless  
and
 
128kbps
 
mp3
 
with
 
hardcutoff
 
around
 
16~17khz,
 
we
 
could
 
see
 
that
 
bias
 
in
 
separated
 
audio
 
even
 
when
 
it
 
was
 
lossless
 
input.
 
Maybe
 
it's
 
a
 
question
 
of
 
balance
 
between
 
lossless/compressed
 
content.
 
I
 
remember
 
the
 
first
 
Ripple
 
bsroformer
 
outputs
 
with
 
these
 
incoherencies
 
in
 
high
 
frequencies
 
 
while
 
we
 
knew
 
it
 
was
 
trained
 
on
 
mixed
 
lossless/128kbps
 
mp3”
 
-
 
jarredou
 
Bas:
 
diversity
 
is
 
key
 
we
 
learned
 
from
 
this
 
paper:
 
https://arxiv.org/pdf/2402.18407
 so  I  take  that  literally,  and  as  a  starting  point.   Q:  so  it  should  also  have  compression  for  it  to  work  better  after  all?  A:  so  diversity  is  also  in  audio  compresion  we  don't  know  for  sure,  but  since  my  model  doesn't  seem  to  perform  bad,  let's  pretend   
D:
 
I’m
 
not
 
sure
 
if
 
it’s
 
really
 
worth
 
to
 
worsen
 
the
 
quality
 
of
 
already
 
lossy
 
stems
 
to
 
create
 
diversity
 
in
 
the
 
dataset
 
artificially.
 
Yes,
 
the
 
model
 
might
 
behave
 
better
 
at
 
lossy
 
inputs,
 
but
 
people
 
should
 
use
 
lossy
 
inputs
 
only
 
occasionally,
 
so
 
I
 
wouldn’t
 
sacrifice
 
quality
 
of
 
lossless
 
inputs
 
that
 
way,
 
plus
 
dataset
 
“can
 
be
 
degraded
 
in
 
many
 
ways
 
on
 
the
 
fly
 
during
 
training
 
with
 
augmentations
 
if
 
needed”.
  Q:  Can  the  training  files  for  dataset  be  mp3  I  added  over  2k  tracks  and  deleted  the  metadata,  it  keeps  only  scanning  the  original  2k  
tracks,
 
not
 
the
 
4k+
 A:  “I  think  you  can  add  "mp3"  extension  to  the  list  there  in  dataset.py  
_____________________  For  help  and  discussion,  visit  our  Audio  Separation  Discord:  https://discord.gg/ZPtAU5R6rP |  Download  UVR or MSST-GUI  For  inst/voc  separation  in  cloud,  try  out  free  Colabs:  BS/Mel-Roformer
 |  MDX23 (2-4  stems)  |  MDX-Net |  VR |  Demucs  4 (2-6)  